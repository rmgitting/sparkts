{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function unpacks the timeseries depending on the inner and global granularities attribute\n",
    "# It can take more than one files, it works with parquet files\n",
    "def unpack(base_dir, paths, drop_all_filled_nulls=True):\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        p = path\n",
    "        df = spark.read.option('basePath', base_dir + '/').parquet(p).sort('timestamp_interval')\n",
    "        \n",
    "        # Extract the measurements and data columns\n",
    "        columns = [col for col in df.columns if 'granularity' not in col and 'timestamp' not in col]\n",
    "        \n",
    "        # Get the granularity and the first timestamp\n",
    "        step = df.select('inner_granularity').first()[0]\n",
    "        first_timestamp = df.select('timestamp_first').first()[0]\n",
    "        \n",
    "        # Combine all data into a temporary column called 'new', and then explode this column\n",
    "        df1 = df.withColumn('new', arrays_zip(*columns))\n",
    "        df1 = df1.select('timestamp_first', 'new')\n",
    "        df1 = df1.withColumn('new', explode('new'))\n",
    "        \n",
    "        # Redistribute the columns in a right way\n",
    "        for c in columns:\n",
    "            column_name = 'new.' + c\n",
    "            df1 = df1.withColumn(c, col(column_name))\n",
    "        df1 = df1.drop('new')\n",
    "        \n",
    "        # Give an incrementally increasing counter to each row\n",
    "        df1 = df1.withColumn('counter', monotonically_increasing_id())\n",
    "        \n",
    "        if drop_all_filled_nulls is True:\n",
    "            df1 = df1.filter(col(columns[0]) != 'filled_null')\n",
    "        # Calculate the interval between the observations in seconds and calculate the exact timestamp \n",
    "        #  for each observation\n",
    "        inSec = calculate_step_from_string(step)\n",
    "        df1 = df1.withColumn('inSec', lit(inSec))\n",
    "        df1 = df1.withColumn('timestamp', (df1.timestamp_first + (df1.counter * df1.inSec)).cast('long'))\n",
    "        df1 = df1.drop('timestamp_first', 'counter', 'inSec')\n",
    "        \n",
    "        # Add the dataframe to the list\n",
    "        dfs.append(df1)\n",
    "    \n",
    "    # Combine all dataframes of the different files\n",
    "    df = reduce(DataFrame.union, dfs)\n",
    "    \n",
    "    return df.sort('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_store(input_path, output_path, inferSchema=False, header=True, timestampFormat='yyyy-MM-dd HH:mm:ss',\n",
    "                   timestampColumnName='timestamp', inner_granularity='1 minute', \n",
    "                   fill_skipped=True, fill_mode='null', global_granularity='1 month'):\n",
    "    \n",
    "    # Read the file from the system\n",
    "    df = spark.read.csv(input_path, inferSchema=inferSchema, header=header, timestampFormat=timestampFormat)\n",
    "    \n",
    "    if timestampColumnName != 'timestamp':\n",
    "        df = df.withColumnRenamed(timestampColumnName, 'timestamp')\n",
    "    df = df.withColumn('timestamp', to_timestamp('timestamp', timestampFormat))      \n",
    "    # Checking and changing column names because Spark does not take columns with dot in them\n",
    "    df = df.toDF(*change_column_names(df.columns))\n",
    "    \n",
    "    \n",
    "    # Change the timestamp to epoch\n",
    "    df = df.withColumn(\"timestamp_epoch\", df.timestamp.cast(\"long\")).sort('timestamp_epoch')\n",
    "    \n",
    "    # Using the concat_udf to merge together the lat and lon\n",
    "    concat_udf = udf(concat, StringType())\n",
    "    \n",
    "    cols = [c for c in df.columns if not c.startswith(\"timestamp\") and not c.startswith(\n",
    "        \"lat\") and not c.startswith(\"lon\")]\n",
    "    coords = [c for c in df.columns if c.startswith(\"lat\") or c.startswith(\"lon\")]\n",
    "    df1 = df.select('timestamp_epoch', *cols, concat_udf(*coords).alias('coordinates'))\n",
    "    \n",
    "    # Dropping the rows with the same timestamp\n",
    "    df1 = df1.drop_duplicates(['timestamp_epoch']).sort('timestamp_epoch')\n",
    "    \n",
    "    # Getting the inner granularity in seconds\n",
    "    step = calculate_step_from_string(inner_granularity)\n",
    "    \n",
    "    \n",
    "    # Optimally, the attribute fill_skipped should be True, we only support filling skipped with nulls now\n",
    "    # and we will still create other data-driven approaches to fill the skipped values\n",
    "    df_joined = df1.withColumn('timestamp', df1.timestamp_epoch)\n",
    "    df_joined = df_joined.drop('timestamp_epoch')\n",
    "    if fill_skipped is True:\n",
    "        minp, maxp = df1.select(min(\"timestamp_epoch\"), max(\"timestamp_epoch\")).first()\n",
    "        reference = spark.range(\n",
    "            (minp / step) * step, ((maxp / step) + 1) * step, step).select(\n",
    "            col(\"id\").cast(\"timestamp\").cast(\"long\").alias(\"timestamp\"))\n",
    "        \n",
    "        if fill_mode == 'null':\n",
    "            # Fill the skipped values with nulls\n",
    "            df_joined = reference.join(df1,reference.timestamp == df1.timestamp_epoch, \"leftouter\").drop(\n",
    "                'timestamp_epoch')\n",
    "            df_joined = df_joined.na.fill('filled_null')\n",
    "        \n",
    "    # Adding the global timestamp to group by it\n",
    "    global_step = calculate_step_from_string(global_granularity)\n",
    "    tsGroup = (floor(df_joined.timestamp / lit(global_step)) * lit(global_step)).alias('timestamp_interval')\n",
    "        \n",
    "    # A lambda to get the first value from an array\n",
    "    getFirst = udf(lambda x:x[0], StringType())\n",
    "    \n",
    "    # Collect the different observations in lists, in order to store them\n",
    "    columns = cols + ['coordinates']\n",
    "    a = [collect_list(c).alias(c) for c in columns]\n",
    "        \n",
    "    # Create the dataframe to save when all observations are packed depending on the global granularity\n",
    "    df2 = (df_joined.groupBy(tsGroup).agg(\n",
    "    *a, collect_list('timestamp').alias('timestamp_first'))).withColumn(\n",
    "        'timestamp_first', getFirst('timestamp_first')).orderBy('timestamp_first')\n",
    "    \n",
    "    # Add the granularities meta-data\n",
    "    df2 = df2.withColumn('global_granularity', lit(global_granularity))\n",
    "    df2 = df2.withColumn('inner_granularity', lit(inner_granularity))\n",
    "    \n",
    "    # partition by the global granularity and store the data as parquet\n",
    "    df2.write.mode('overwrite').partitionBy('timestamp_interval').parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
